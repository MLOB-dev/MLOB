
# MultiLevelOptimalBayes

<!-- badges: start -->
<!-- badges: end -->

# MultiLevelOptimalBayes (MLOB) <img src="https://img.shields.io/badge/R-CRAN-blue.svg" alt="R badge" />

**MLOB** is an R package for estimating between-group effects in
multilevel latent variable models using an optimally regularized
Bayesian estimator. It is especially useful for small-sample settings,
low ICC data, and hierarchical models commonly used in psychology,
education, and social sciences.

------------------------------------------------------------------------

## âœ¨ Features

- âœ… **Regularized Bayesian estimator** optimized for minimum mean
  squared error (MSE)
- âœ… **Robust to small sample sizes** and **low intraclass correlation
  (ICC)**
- âœ… Supports **covariate adjustment** and **group-level balancing**
- âœ… Optional **jackknife resampling** for improved CI coverage
- âœ… Returns **full inferential output**: point estimates, standard
  errors, p-values, and confidence intervals
- âœ… Clean and user-friendly interface via the `mlob()` function

------------------------------------------------------------------------

## ğŸ“¦ Installation

To install the development version from GitHub:

\`\`\`r install.packages(â€œdevtoolsâ€)

MLOB is available on CRAN under the GPL-3 license. To install the
released version: \`\`\`r install.packages(â€œMLOBâ€)

The development version can be installed from GitHub via devtools:
\`\`\`r devtools::install_github(â€œDfkthbq01/MultiLevelOptimalBayesâ€)

## ğŸ“¦ View the Vignette

After installing the package, run the following to open the introductory
vignette:

\`\`\`r vignette(â€œMultiLevelOptimalBayes-Introâ€)

## ğŸ“¦ Examples

\`\`\`r

library(MultiLevelOptimalBayes)

### Fit a model on the iris dataset

result \<- mlob( Sepal.Length ~ Sepal.Width + Petal.Length, data = iris,
group = â€œSpeciesâ€, conf.level = 0.95 )

### View results

summary(result)

## ğŸ“¦ Limitations

-The estimator assumes approximately equal group sizes. Although
balancing helps, unequal sizes may still bias results.

- Grid-search is local (Â±5Ïƒ) around the ML estimate; global optimum is
  not guaranteed.

- Linear covariate residualization may miss nonlinear effects or
  interactions.

- Jackknife resampling improves inference in small samples but can be
  computationally heavy.

-Currently supports two-level models with continuous outcomes only.
Extensions to GLMMs or 3+ level models are future work.

## ğŸ“¦ Contributing & Support

Please open an issue at:

<https://github.com/Dfkthbq01/MultiLevelOptimalBayes/issues>

Users may also join discussions or suggest enhancements on the
Discussions page at

<https://github.com/Dfkthbq01/MultiLevelOptimalBayes/discussions>.

## ğŸ“¦Authors

Valerii Dashuk

Binayak Timilsina

Martin Hecht

Steffen Zitzmann

## ğŸ“š Citation

If you use MLOB in your research, please cite:

Dashuk, V., Hecht, M., Luedtke, O., Robitzsch, A., & Zitzmann, S.
(2024). An Optimally Regularized Estimator of Multilevel Latent Variable
Models with Improved MSE Performance.
<https://doi.org/10.13140/RG.2.2.18148.39048>

## ğŸ“« Contact:

<martin.hecht@hsu-hh.de>

<steffen.zitzmann@medicalschool-hamburg.de>

<multilob@outlook.com>
